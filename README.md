# E2NGIADINA RAG System

Ein vollstÃ¤ndiges RAG (Retrieval-Augmented Generation) System mit GPU-UnterstÃ¼tzung und erweiterter Datei-Indexierung.

## ğŸš€ Quick Start

**Einzelner Befehl zum Starten des gesamten Systems:**
```bash
cd /media/felix/RAG/AGENTIC
./scripts/start_all.sh
```

## ğŸ“‹ SystemÃ¼bersicht

### Features
- **GPU-beschleunigtes LLM**: llama4:latest auf NVIDIA GB10
- **Multi-Format Indexierung**: PDF, DOCX, XLSX, MSG, PPTX, TXT, HTML, CSV, JSON, XML, YAML, ZIP
- **Vector Database**: ChromaDB mit 50,000+ Dokumenten
- **Web Interface**: OpenWebUI auf Port 8086
- **REST API**: OpenAI-kompatibel auf Port 11436

### Services
| Service | Port | Beschreibung |
|---------|------|-------------|
| OpenWebUI | 8086 | Web Interface |
| Agent API | 11436 | RAG API |
| Ollama | 11434 | LLM Inference (GPU) |
| Runner | 9000 | Code Execution |

## ğŸ› ï¸ Installation & Setup

### 1. Voraussetzungen
- Docker & Docker Compose
- NVIDIA GPU mit CUDA 12.1+
- 7,985+ Dateien in `/media/felix/RAG/1`

### 2. System starten
```bash
cd /media/felix/RAG/AGENTIC
chmod +x scripts/*.sh
./scripts/start_all.sh
```

### 3. Daten indexieren
```bash
docker compose run --rm indexer
```

### 4. System testen
```bash
# Health Check
curl -s http://localhost:11436/health

# API Test
curl -s -X POST http://localhost:11436/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model":"agentic-rag","messages":[{"role":"user","content":"test query"}]}'

# Web Interface
# Ã–ffne http://localhost:8086 im Browser
```

## ğŸ“ Projektstruktur

```
/media/felix/RAG/AGENTIC/
â”œâ”€â”€ README.md                    # Diese Datei
â”œâ”€â”€ WINDSURF_SETUP.md           # Detaillierte Dokumentation
â”œâ”€â”€ docker-compose.yml          # Service-Konfiguration
â”œâ”€â”€ scripts/                    # Automatisierungsskripte
â”‚   â”œâ”€â”€ start_all.sh           # Komplett-Start
â”‚   â”œâ”€â”€ status_check.sh        # System-Status
â”‚   â”œâ”€â”€ reset_system.sh        # Komplett-Reset
â”‚   â”œâ”€â”€ ingest_pdfs.sh         # PDF-Indexierung (legacy)
â”‚   â””â”€â”€ smoke_test.sh          # API-Tests
â”œâ”€â”€ agent_api/                  # RAG API Service
â”œâ”€â”€ indexer/                    # Multi-Format Indexierung
â”œâ”€â”€ runner/                     # Code Execution Service
â””â”€â”€ volumes/                    # Persistente Daten
    â”œâ”€â”€ chroma/                 # Vector Database
    â”œâ”€â”€ ollama/                 # LLM Models
    â”œâ”€â”€ manifest/               # Index Manifest
    â””â”€â”€ logs/                   # System Logs
```

## ğŸ”§ Management Scripts

### System starten
```bash
./scripts/start_all.sh
```
- Setzt Berechtigungen
- Startet alle Services
- LÃ¤dt llama4:latest Modell
- FÃ¼hrt Health-Checks durch

### System-Status prÃ¼fen
```bash
./scripts/status_check.sh
```
- Zeigt Service-Status
- GPU-Auslastung
- ChromaDB Dokumentenzahl
- Speichernutzung

### System zurÃ¼cksetzen
```bash
./scripts/status_check.sh
```
- Stoppt alle Services
- Bereinigt Docker-Ressourcen
- Optionales LÃ¶schen von Daten

## ğŸ“Š Monitoring & Debugging

### Logs ansehen
```bash
# Alle Services
docker compose logs --tail 100

# Spezifische Services
docker logs --tail 100 agentic-api
docker logs --tail 100 agentic-ollama

# Anwendungslogs
tail -f ./volumes/logs/indexer.log
tail -f ./volumes/logs/agent_api.log
```

### GPU-Status
```bash
nvidia-smi
```

### ChromaDB Statistik
```bash
docker compose run --rm indexer python -c "
import chromadb
c=chromadb.PersistentClient('/chroma')
col=c.get_or_create_collection('documents')
print('Dokumente:', col.count())
"
```

## ğŸ”„ Datenverarbeitung

### UnterstÃ¼tzte Formate
- **PDF**: PyMuPDF
- **Office**: DOCX, XLSX, PPTX
- **Email**: MSG (Outlook)
- **Web**: HTML, XML
- **Daten**: CSV, JSON, YAML
- **Text**: TXT, MD
- **Archive**: ZIP (recursive, depth=2)

### Verarbeitungs-Pipeline
1. Datei-Discovery in `/media/felix/RAG/1`
2. Content-Extraktion via `text_loaders.py`
3. Text-Chunking (1200 chars, 180 overlap)
4. Embedding-Generierung (all-MiniLM-L6-v2)
5. Speicherung in ChromaDB
6. Manifest-Tracking fÃ¼r Updates

## ğŸŒ API-Nutzung

### OpenAI-kompatibles Endpunkt
```bash
curl -X POST http://localhost:11436/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "agentic-rag",
    "messages": [{"role": "user", "content": "Ihre Frage hier"}]
  }'
```

### Web Interface
- **URL**: http://localhost:8086
- **Modell**: agentic-rag
- **Funktionen**: Chat mit RAG, Dokumenten-Suche

## ğŸ› ï¸ Fehlersuche

### HÃ¤ufige Probleme
1. **Port-Konflikte**: Ports 8086, 9000, 11434, 11436 frei?
2. **GPU nicht erkannt**: `nvidia-smi` prÃ¼fen
3. **Speicherprobleme**: RAM wÃ¤hrend Indexierung Ã¼berwachen
4. **Berechtigungen**: `chmod +x scripts/*.sh` ausfÃ¼hren

### Kompletter Reset
```bash
./scripts/reset_system.sh
./scripts/start_all.sh
```

## ğŸ“ˆ Performance

- **GPU**: NVIDIA GB10, CUDA 12.1
- **Modell**: llama4:latest (GPU-beschleunigt)
- **Verarbeitung**: 6 Worker, Batch-Size 256
- **Speicher**: 24h Model-Keep-Alive
- **Dokumente**: 50,000+ in ChromaDB

## ğŸ” Sicherheit

- CORS fÃ¼r Entwicklung konfiguriert
- Kein Internet-Zugriff fÃ¼r Runner-Container
- Daten-Verzeichnis read-only gemountet
- API-Keys fÃ¼r lokalen Setup leer konfiguriert

## ğŸ“ Support

1. **Logs prÃ¼fen**: `./scripts/status_check.sh`
2. **GPU prÃ¼fen**: `nvidia-smi`
3. **Services neustarten**: `docker compose restart`
4. **Kompletter Reset**: `./scripts/reset_system.sh`

---

**WINDSURF RAG System** - Production-ready mit GPU-Beschleunigung und erweiterter Datei-Indexierung.

import os
import time
import hashlib
import json
import asyncio
import traceback
from fastapi import FastAPI, Header, Request, HTTPException
from fastapi.responses import FileResponse, StreamingResponse, JSONResponse
from pydantic import BaseModel

from .agent_orchestrator import Agent, AgentOrchestrator
from .rag_pipeline import SimpleRAGPipeline
from .state import StateStore
from .es_proxy import router as es_router

app = FastAPI(title="AGENTIC RAG API")

ollama_base = os.getenv("OLLAMA_BASE_URL", "http://ollama:11434")

# Agent instances
agent = Agent()
orchestrator = AgentOrchestrator()

# MVP: Simple RAG pipeline (Query ‚Üí Search ‚Üí Snippets ‚Üí LLM)
simple_rag = SimpleRAGPipeline()

ENABLE_DEBUG_ENDPOINTS = os.getenv("ENABLE_DEBUG_ENDPOINTS", "0") == "1"

STATE_PATH = os.getenv("STATE_PATH", "/state")
store = StateStore(STATE_PATH)

# ----------------------------
# OpenAI-compatible SSE helpers
# ----------------------------

def _normalize_delta_content(v) -> str:
    """
    OpenAI streaming requires: choices[0].delta.content is a STRING (or absent).
    Some internal code may emit dicts like {"type":"token","content":"..."}.
    We convert those to strings so OpenWebUI can render streaming properly.
    """
    if v is None:
        return ""
    if isinstance(v, str):
        return v
    if isinstance(v, bytes):
        try:
            return v.decode("utf-8", errors="ignore")
        except Exception:
            return str(v)
    if isinstance(v, dict):
        # common internal shape: {"type": "...", "content": "..."}
        inner = v.get("content")
        if isinstance(inner, str):
            return inner
        if inner is None:
            # fallback: stringify dict (last resort)
            return json.dumps(v, ensure_ascii=False)
        return str(inner)
    # fallback: stringify any other type
    return str(v)

def _sse_chunk(rid: str, created: int, model: str, delta: dict, finish_reason=None) -> str:
    safe_delta = {}
    if "role" in delta and isinstance(delta["role"], str):
        safe_delta["role"] = delta["role"]
    if "content" in delta:
        safe_delta["content"] = _normalize_delta_content(delta["content"])

    chunk = {
        "id": rid,
        "object": "chat.completion.chunk",
        "created": created,
        "model": model,
        "choices": [{
            "index": 0,
            "delta": safe_delta,
            "finish_reason": finish_reason
        }]
    }
    return f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"

def _format_phase_for_ui(event: dict) -> str:
    """Format phase events for OpenWebUI display"""
    event_type = event.get("type")
    phase = event.get("phase", "")
    
    if event_type == "phase_start":
        phase_names = {
            "strategy": "üéØ Strategie",
            "retrieval": "üîç Suche", 
            "analysis": "üìÑ Dokumenten-Analyse",
            "validation": "‚úì Validierung",
            "answer": "üí° Antwort"
        }
        name = phase_names.get(phase, phase)
        return f"\n[{name}] wird gestartet...\n"
    
    elif event_type == "phase_progress":
        message = event.get("message", "")
        return f"  ‚Üí {message}\n"
    
    elif event_type == "phase_complete":
        return f"  ‚úì Abgeschlossen\n"
    
    elif event_type == "error":
        msg = event.get("message", "Unknown error")
        return f"  ‚ö† Fehler: {msg}\n"
    
    return ""

def derive_conv_id(messages, x_conversation_id: str | None) -> str:
    if x_conversation_id and x_conversation_id.strip():
        return x_conversation_id.strip()
    joined = "\n".join([f"{m.role}:{m.content}" for m in messages])[:2000]
    h = hashlib.sha1(joined.encode("utf-8", errors="ignore")).hexdigest()
    return f"conv_{h[:16]}"

# ----------------------------
# OpenAI request models
# ----------------------------

class Message(BaseModel):
    role: str
    content: str

class RAGConfig(BaseModel):
    max_context_docs: int | None = None
    max_sources: int | None = None
    search_top_k: int | None = None
    keyword_boost_path: float | None = None
    keyword_boost_snippet: float | None = None
    excel_penalty_relevant: float | None = None
    excel_penalty_irrelevant: float | None = None
    answer_temperature: float | None = None

class ChatReq(BaseModel):
    model: str | None = None
    messages: list[Message]
    stream: bool | None = False
    rag_config: RAGConfig | None = None

# ----------------------------
# Endpoints
# ----------------------------

@app.get("/health")
def health():
    return {
        "ok": True,
        "service": "agent_api",
        "version": "2.0-multi-phase",
        "time": int(time.time()),
        "models": {
            "strategy": orchestrator.model_strategy,
            "answer": orchestrator.model_answer
        }
    }

@app.get("/v1/models")
async def models():
    """List available RAG models (ollama models with rag- prefix)"""
    try:
        # Fetch models from Ollama
        import httpx
        async with httpx.AsyncClient(timeout=10.0) as client:
            r = await client.get(f"{ollama_base}/api/tags")
            ollama_models = r.json().get("models", [])
        
        # Format for OpenAI-compatible response with rag- prefix
        # Skip embedding models (not usable for chat)
        # Add -think variants for models >= 14B
        skip_keywords = ["embed", "embedding"]
        think_min_size = 10 * 1e9  # 10GB minimum for thinking mode
        data = []
        for m in ollama_models:
            model_id = m.get("name", m.get("model", ""))
            if any(kw in model_id.lower() for kw in skip_keywords):
                continue
            rag_model_id = f"rag-{model_id}"
            data.append({
                "id": rag_model_id,
                "object": "model",
                "created": 0,
                "owned_by": "rag-pipeline"
            })
            # Add thinking variant for larger models
            model_size = m.get("size", 0)
            if model_size >= think_min_size:
                data.append({
                    "id": f"rag-{model_id}-think",
                    "object": "model",
                    "created": 0,
                    "owned_by": "rag-pipeline-think"
                })
        
        return {"object": "list", "data": data}
    except Exception as e:
        # Fallback to static list if Ollama unreachable
        return {
            "object": "list",
            "data": [
                {"id": "rag-llama4:latest", "object": "model", "created": 0, "owned_by": "rag-pipeline"},
                {"id": "rag-apertus:70b-instruct-2509-q4_k_m", "object": "model", "created": 0, "owned_by": "rag-pipeline"},
                {"id": "rag-qwen2.5:3b", "object": "model", "created": 0, "owned_by": "rag-pipeline"},
                {"id": "rag-gpt-oss:latest", "object": "model", "created": 0, "owned_by": "rag-pipeline"},
            ]
        }

@app.get("/open")
async def open_file(path: str):
    file_base = os.getenv("FILE_BASE", "")
    if file_base and not path.startswith(file_base):
        return {"error": "Access denied - path outside base directory"}

    normalized_path = os.path.normpath(path)
    if not os.path.exists(normalized_path):
        return {"error": "File not found"}
    if not os.path.isfile(normalized_path):
        return {"error": "Path is not a file"}

    return FileResponse(normalized_path)

async def chat_non_stream_impl(req: ChatReq, x_conversation_id: str | None = None):
    if not req.messages:
        return {"error": "No messages provided"}

    conv_id = derive_conv_id(req.messages, x_conversation_id)
    state = store.load(conv_id)
    summary = state.get("summary", "") or ""
    notes = state.get("notes", "") or ""

    user_text = ""
    for m in req.messages[::-1]:
        if m.role == "user":
            user_text = m.content
            break
    
    # Pipeline selection: Default = SimpleRAG (MVP), [ADVANCED] = Agentic
    use_simple_rag = True
    if user_text.startswith("[ADVANCED]"):
        use_simple_rag = False
        user_text = user_text.replace("[ADVANCED]", "").strip()
    
    if use_simple_rag:
        # MVP: Simple RAG Pipeline - use selected model (strip rag- prefix)
        selected_model = (req.model or "llama4:latest").replace("rag-", "", 1)
        
        # Build per-request config dict from rag_config
        run_config = {}
        if req.rag_config:
            if req.rag_config.max_context_docs is not None:
                run_config["max_context_docs"] = req.rag_config.max_context_docs
            if req.rag_config.max_sources is not None:
                run_config["max_sources"] = req.rag_config.max_sources
            if req.rag_config.search_top_k is not None:
                run_config["search_top_k"] = req.rag_config.search_top_k
            if req.rag_config.keyword_boost_path is not None:
                run_config["keyword_boost_path"] = req.rag_config.keyword_boost_path
            if req.rag_config.keyword_boost_snippet is not None:
                run_config["keyword_boost_snippet"] = req.rag_config.keyword_boost_snippet
            if req.rag_config.excel_penalty_relevant is not None:
                run_config["excel_penalty_relevant"] = req.rag_config.excel_penalty_relevant
            if req.rag_config.excel_penalty_irrelevant is not None:
                run_config["excel_penalty_irrelevant"] = req.rag_config.excel_penalty_irrelevant
            if req.rag_config.answer_temperature is not None:
                run_config["answer_temperature"] = req.rag_config.answer_temperature
        
        # Create pipeline with selected model
        from .rag_pipeline import create_pipeline
        pipeline = create_pipeline("simple", model=selected_model)
        
        answer_parts = []
        sources = []
        
        async for event in pipeline.run(user_text, summary, notes, config=run_config):
            if event.type == "token":
                answer_parts.append(event.data.get("content", ""))
            elif event.type == "complete":
                sources = event.data.get("sources", [])
        
        answer = "".join(answer_parts) if answer_parts else "Keine Antwort generiert."
        
        # Add sources directly to answer content as markdown links
        if sources:
            lines = ["", "", "Quellen:"]
            for s in sources:
                n = s.get("n", "?")
                dp = s.get("display_path", s.get("path", ""))
                url = s.get("local_url", "")
                if url:
                    lines.append(f"[{n}] [{dp}]({url})")
                else:
                    lines.append(f"[{n}] {dp}")
            answer += "\n" + "\n".join(lines)
        
        # Build sources for response
        formatted_sources = []
        for s in sources:
            formatted_sources.append({
                "n": s.get("n", "?"),
                "path": s.get("path", ""),
                "display_path": s.get("display_path", s.get("path", ""))
            })
        
        response = {
            "id": f"agentic_{int(time.time())}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": req.model or "agentic-rag",
            "choices": [{
                "index": 0,
                "message": {"role": "assistant", "content": answer},
                "finish_reason": "stop"
            }]
        }
        if formatted_sources:
            response["sources"] = formatted_sources
        
        # Save state (simple rag doesn't update summary/notes yet)
        store.save(conv_id, summary, notes)
        
        return response
    else:
        # Advanced: Agentic pipeline with full analysis
        answer, new_summary, new_notes, sources = await agent.answer(
            user_text=user_text,
            raw_messages=[m.model_dump() for m in req.messages],
            summary=summary,
            notes=notes,
        )

        store.save(conv_id, new_summary, new_notes)

        response = {
            "id": f"agentic_{int(time.time())}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": req.model or "agentic-rag",
            "choices": [{
                "index": 0,
                "message": {"role": "assistant", "content": answer},
                "finish_reason": "stop"
            }]
        }
        if sources:
            response["sources"] = sources
        return response

@app.post("/v1/chat/completions")
async def chat(req: ChatReq, request: Request, x_conversation_id: str | None = Header(default=None)):
    rid = f"agentic_{int(time.time())}"
    created = int(time.time())
    model = req.model or "agentic-rag"
    
    if getattr(req, "stream", False):
        async def gen():
            # Send first chunk immediately
            yield _sse_chunk(rid, created, model, {"role": "assistant"})
            
            try:
                user_text = next((m.content for m in req.messages[::-1] if m.role == "user"), "")
                
                # MVP: SimpleRAG with selected model (strip rag- prefix and -think suffix)
                selected_model = model.replace("rag-", "", 1) if model.startswith("rag-") else (model or "llama4:latest")
                selected_model = selected_model.replace("-think", "")
                
                # Resolve conversation ID for session state
                conv_id = x_conversation_id or hashlib.md5(user_text.encode()).hexdigest()[:12]
                session = store.load(conv_id)
                summary = session.get("summary", "")
                notes = session.get("notes", "")
                
                # Check if user references multiple previous sources: "diesen Dokumenten", "den Quellen"
                from .source_analyzer import detect_source_reference, detect_multi_source_reference, fetch_document_text
                multi_ref = detect_multi_source_reference(user_text)
                
                if multi_ref is not None:
                    # Load previous sources
                    last_session = store.load("last_sources")
                    prev_sources = last_session.get("sources", [])
                    
                    if prev_sources:
                        # Build conversation history for context
                        chat_history = []
                        for m in req.messages[:-1]:
                            if m.role in ("user", "assistant") and m.content:
                                content = m.content[:2000] if len(m.content) > 2000 else m.content
                                chat_history.append({"role": m.role, "content": content})
                        chat_history = chat_history[-6:]
                        
                        # Fetch full text for each previous source (max 5)
                        max_docs = min(len(prev_sources), 5)
                        doc_texts = []
                        loaded_sources = []
                        source_names = []
                        
                        yield _sse_chunk(rid, created, model, {"content": f"üìÑ Lade {max_docs} Dokumente f√ºr Detailanalyse...\n\n"})
                        
                        for i, src in enumerate(prev_sources[:max_docs], 1):
                            src_path = src.get("path", "")
                            src_display = src.get("display_path", src_path)
                            doc_text, _ = await fetch_document_text(src_path)
                            if doc_text:
                                # Truncate per document
                                max_per_doc = 8000
                                if len(doc_text) > max_per_doc:
                                    doc_text = doc_text[:max_per_doc] + f"\n[... gek√ºrzt, {len(doc_text)} Zeichen total]"
                                doc_texts.append(f"[{i}] {src_display}:\n{doc_text}")
                                loaded_sources.append(src)
                                source_names.append(f"[{i}] {src_display}")
                        
                        if doc_texts:
                            print(f"üìÑ Multi-source analysis: {len(doc_texts)} docs loaded")
                            
                            combined_context = "\n\n---\n\n".join(doc_texts)
                            
                            system_prompt = """DU BIST EIN DOKUMENTEN-ANALYSE-SYSTEM F√úR SCHWEIZER EISENBAHN-PROJEKTE (SBB TFK 2020).

REGELN:
1. Antworte IMMER auf Deutsch
2. Analysiere ALLE bereitgestellten Dokumente VOLLST√ÑNDIG und GR√úNDLICH
3. Zitiere jede Aussage mit [N] (Dokumentnummer)
4. Sei EXHAUSTIV - liste ALLE gefundenen Eintr√§ge auf, nicht nur die ersten paar
5. Nutze Tabellen, Aufz√§hlungen und √úberschriften f√ºr √úbersichtlichkeit
6. Erfinde nichts - nur was in den Dokumenten steht"""

                            user_msg = f"""Hier sind die Dokumente:

{combined_context}

Aufgabe: {user_text}"""

                            messages = [
                                {"role": "system", "content": system_prompt},
                            ]
                            if chat_history:
                                messages.extend(chat_history)
                            messages.append({"role": "user", "content": user_msg})
                            
                            from .rag_pipeline import SimpleRAGPipeline
                            pipeline = SimpleRAGPipeline(model=selected_model)
                            
                            async for chunk in pipeline._llm_stream(messages):
                                yield _sse_chunk(rid, created, model, {"content": chunk})
                            
                            # Add source links
                            from urllib.parse import quote
                            source_lines = []
                            for j, src in enumerate(loaded_sources, 1):
                                dp = src.get("display_path", src.get("path", ""))
                                url = src.get("local_url", "")
                                if url:
                                    source_lines.append(f"[{j}] [{dp}]({url})")
                                else:
                                    source_lines.append(f"[{j}] {dp}")
                            yield _sse_chunk(rid, created, model, {"content": "\n\nQuellen:\n" + "\n".join(source_lines)})
                        else:
                            yield _sse_chunk(rid, created, model, {"content": "‚ö†Ô∏è Dokumentinhalte konnten nicht aus ES geladen werden.\n"})
                        
                        yield _sse_chunk(rid, created, model, {"content": ""}, finish_reason="stop")
                        yield "data: [DONE]\n\n"
                        return
                
                # Check if user references a previous source: "Analysiere Quelle [1]"
                source_ref = detect_source_reference(user_text)
                
                if source_ref is not None:
                    # Load previous sources from global last_sources
                    last_session = store.load("last_sources")
                    prev_sources = last_session.get("sources", [])
                    
                    if prev_sources and 1 <= source_ref <= len(prev_sources):
                        ref_source = prev_sources[source_ref - 1]
                        ref_path = ref_source.get("path", "")
                        ref_display = ref_source.get("display_path", ref_path)
                        
                        yield _sse_chunk(rid, created, model, {"content": f"üìÑ Analysiere Dokument: **{ref_display}**\n\n"})
                        
                        # Fetch full text from ES
                        doc_text, doc_meta = await fetch_document_text(ref_path)
                        
                        if not doc_text:
                            yield _sse_chunk(rid, created, model, {"content": "‚ö†Ô∏è Dokumentinhalt konnte nicht aus ES geladen werden.\n"})
                        else:
                            # Truncate if too long for LLM context
                            max_chars = 12000
                            if len(doc_text) > max_chars:
                                doc_text = doc_text[:max_chars] + f"\n\n[... gek√ºrzt, {len(doc_text)} Zeichen total]"
                            
                            # Stream LLM analysis
                            from .rag_pipeline import SimpleRAGPipeline
                            pipeline = SimpleRAGPipeline(model=selected_model)
                            
                            system_prompt = """DU BIST EIN DOKUMENTEN-ANALYSE-SYSTEM F√úR SCHWEIZER EISENBAHN-PROJEKTE (SBB TFK 2020).

REGELN:
1. Antworte IMMER auf Deutsch
2. Fasse den Dokumentinhalt strukturiert und vollst√§ndig zusammen
3. Hebe wichtige Fakten, Daten, Personen und Entscheidungen hervor
4. Nutze Aufz√§hlungen und √úberschriften f√ºr √úbersichtlichkeit
5. Sei pr√§zise und faktenbasiert - erfinde nichts"""

                            user_msg = f"""Hier ist das Dokument "{ref_display}":

{doc_text}

Aufgabe: {user_text}"""

                            messages = [
                                {"role": "system", "content": system_prompt},
                                {"role": "user", "content": user_msg}
                            ]
                            
                            async for chunk in pipeline._llm_stream(messages):
                                yield _sse_chunk(rid, created, model, {"content": chunk})
                            
                            # Add source link at end
                            from urllib.parse import quote
                            url = ref_source.get("local_url", "")
                            yield _sse_chunk(rid, created, model, {"content": f"\n\nQuelle: [{ref_display}]({url})"})
                        
                        yield _sse_chunk(rid, created, model, {"content": ""}, finish_reason="stop")
                        yield "data: [DONE]\n\n"
                        return
                    else:
                        yield _sse_chunk(rid, created, model, {"content": f"‚ö†Ô∏è Quelle [{source_ref}] nicht gefunden. Bitte zuerst eine Suche durchf√ºhren.\n"})
                        yield _sse_chunk(rid, created, model, {"content": ""}, finish_reason="stop")
                        yield "data: [DONE]\n\n"
                        return
                
                # Normal RAG flow
                # Build per-request config from rag_config if provided
                run_config = {}
                if hasattr(req, 'rag_config') and req.rag_config:
                    if req.rag_config.max_context_docs is not None:
                        run_config["max_context_docs"] = req.rag_config.max_context_docs
                    if req.rag_config.max_sources is not None:
                        run_config["max_sources"] = req.rag_config.max_sources
                
                # Build conversation history from previous messages (for follow-up context)
                chat_history = []
                for m in req.messages[:-1]:  # All messages except the last (current) one
                    if m.role in ("user", "assistant") and m.content:
                        # Truncate long messages to keep context manageable
                        content = m.content[:2000] if len(m.content) > 2000 else m.content
                        chat_history.append({"role": m.role, "content": content})
                # Keep only last 6 messages (3 turns) to avoid context overflow
                chat_history = chat_history[-6:]
                
                # For follow-ups: load previous source documents for context
                prev_doc_context = ""
                if chat_history:
                    last_session = store.load("last_sources")
                    prev_sources = last_session.get("sources", [])
                    if prev_sources:
                        # Fetch full text for previous sources (max 3, to limit context size)
                        doc_parts = []
                        for i, src in enumerate(prev_sources[:3], 1):
                            src_path = src.get("path", "")
                            src_display = src.get("display_path", src_path)
                            doc_text, _ = await fetch_document_text(src_path)
                            if doc_text:
                                max_per_doc = 6000
                                if len(doc_text) > max_per_doc:
                                    doc_text = doc_text[:max_per_doc] + f"\n[... gek√ºrzt, {len(doc_text)} Zeichen total]"
                                doc_parts.append(f"[{i}] {src_display}:\n{doc_text}")
                        if doc_parts:
                            prev_doc_context = "\n\n---\n\n".join(doc_parts)
                            print(f"üìé Follow-up: loaded {len(doc_parts)} previous source docs as context")
                
                # Detect thinking mode from model name suffix
                enable_thinking = "-think" in model.lower() or ":think" in model.lower()
                
                # Detect filesystem queries (counting/listing files) ‚Üí inject code-execution instruction
                from .code_executor import detect_filesystem_query, FILESYSTEM_CODE_INSTRUCTION
                fs_hint = detect_filesystem_query(user_text)
                code_instruction = ""
                if fs_hint:
                    code_instruction = FILESYSTEM_CODE_INSTRUCTION
                    print(f"üìÇ Filesystem query detected: {fs_hint}")
                
                print(f"üß† Model='{model}', selected='{selected_model}', thinking={enable_thinking}, history={len(chat_history)} msgs, fs_query={fs_hint is not None}")
                
                from .rag_pipeline import create_pipeline
                pipeline = create_pipeline("simple", model=selected_model)
                
                answer_parts = []
                sources = []
                
                async for event in pipeline.run(user_text, "", "", config=run_config, thinking=enable_thinking, chat_history=chat_history, prev_doc_context=prev_doc_context, code_instruction=code_instruction):
                    if event.type == "token":
                        content = event.data.get("content", "")
                        answer_parts.append(content)
                        yield _sse_chunk(rid, created, model, {"content": content})
                    elif event.type == "complete":
                        sources = event.data.get("sources", [])
                
                # Auto-execute Python code blocks in the answer
                full_answer = "".join(answer_parts)
                from .code_executor import extract_code_blocks, execute_code, format_execution_result
                code_blocks = extract_code_blocks(full_answer)
                if code_blocks:
                    for i, (lang, code) in enumerate(code_blocks):
                        yield _sse_chunk(rid, created, model, {"content": f"\n\n‚öôÔ∏è **Code wird ausgef√ºhrt...**\n"})
                        exec_result = await execute_code(code)
                        formatted = format_execution_result(exec_result)
                        yield _sse_chunk(rid, created, model, {"content": f"\nüìä **Ergebnis:**\n```\n{formatted}\n```\n"})
                
                # Add sources at end with clickable links
                if sources:
                    from urllib.parse import quote
                    source_lines = []
                    for s in sources:
                        n = s.get('n', '?')
                        dp = s.get('display_path', s.get('path', ''))
                        url = s.get('local_url', '')
                        if url:
                            source_lines.append(f"[{n}] [{dp}]({url})")
                        else:
                            source_lines.append(f"[{n}] {dp}")
                    source_text = "\n\nQuellen:\n" + "\n".join(source_lines)
                    yield _sse_chunk(rid, created, model, {"content": source_text})
                
                # Save sources globally for "Analysiere Quelle [N]" feature
                store.save(conv_id, summary, notes)
                if sources:
                    store.save("last_sources", "", "", sources=sources)
                
                # End marker
                yield _sse_chunk(rid, created, model, {"content": ""}, finish_reason="stop")
                yield "data: [DONE]\n\n"

            except Exception as e:
                yield _sse_chunk(rid, created, model, {"content": f"\n‚ö† Fehler: {type(e).__name__}: {e}"}, finish_reason="stop")
                yield "data: [DONE]\n\n"

        headers = {
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        }
        return StreamingResponse(gen(), media_type="text/event-stream", headers=headers)

    # Non-stream path
    full = await chat_non_stream_impl(req, x_conversation_id)
    if isinstance(full, dict) and "error" in full:
        return JSONResponse(full, status_code=400)
    return full

# Include ES Proxy Router
app.include_router(es_router, prefix="/proxy", tags=["es-proxy"])

# Debug SSE endpoint (ChatGPT's debugging suggestion)
@app.get("/debug/sse")
async def debug_sse(request: Request):
    if not ENABLE_DEBUG_ENDPOINTS:
        raise HTTPException(status_code=404, detail="Not found")
    async def gen():
        try:
            # sofort was senden (verhindert OpenWebUI "graue Streifen")
            yield "data: [debug] hello\n\n"
            # heartbeat loop
            while True:
                if await request.is_disconnected():
                    print("[debug_sse] client disconnected")
                    return
                yield f"data: [hb] {int(time.time())}\n\n"
                await asyncio.sleep(2)
        except asyncio.CancelledError:
            print("[debug_sse] cancelled")
            return
        except Exception as e:
            print("[debug_sse] EXC:", repr(e))
            print(traceback.format_exc())
            # noch versuchen, Fehler als SSE zu senden
            yield f"data: [error] {repr(e)}\n\n"

    return StreamingResponse(
        gen(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        },
    )

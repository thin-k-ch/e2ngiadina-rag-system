# ğŸ¯ Multi-Modell Guide fÃ¼r OpenWebUI

## ğŸ“‹ Aktuelle Konfiguration

Ihr OpenWebUI ist jetzt konfiguriert fÃ¼r die gleichzeitige Nutzung von:
- **RAG Agent:** `agentic-rag` (mit Memory und 37.624 Dokumenten)
- **Direkte Ollama Modelle:** `llama4:latest` und `qwen2.5:14b`

## ğŸŒ Zugriff

**Web Interface:** http://localhost:8086

## ğŸ”„ Modell-Wechsel

### Option 1: Ãœber das Web Interface
1. Ã–ffnen Sie http://localhost:8086
2. Klicken Sie auf das Modell-Dropdown oben links
3. WÃ¤hlen Sie zwischen:
   - `agentic-rag` â†’ FÃ¼r RAG mit Memory und Dokumenten
   - `llama4:latest` â†’ FÃ¼r direkte LLM-Anfragen
   - `qwen2.5:14b` â†’ FÃ¼r schnellere Antworten

### Option 2: Automatische Erkennung
OpenWebUI sollte automatisch alle verfÃ¼gbaren Modelle anzeigen.

## ğŸ¯ AnwendungsfÃ¤lle

### ğŸ“š RAG Agent (`agentic-rag`)
**Verwenden fÃ¼r:**
- ğŸ“„ Dokumenten-basierte Fragen
- ğŸ§  Persistente Conversations
- ğŸ” Kontextbezogene Antworten mit Zitaten
- ğŸ“Š Business Intelligence

**Beispiel:**
```
"Welche Offerten hat Rhomberg Bahntechnik fÃ¼r den Gotthard Basistunnel abgegeben?"
```

### ğŸš€ Direkte LLMs (`llama4:latest`, `qwen2.5:14b`)
**Verwenden fÃ¼r:**
- ğŸ’¬ Allgemeine Konversationen
- ğŸ”§ Code-Generierung
- ğŸ“ Text-Erstellung
- ğŸ¤– Kreative Aufgaben

**Beispiel:**
```
"Schreibe ein Python-Skript fÃ¼r Datenanalyse"
```

## âš™ï¸ Konfigurations-Details

### Environment Variablen
```yaml
openwebui:
  environment:
    - OPENAI_API_BASE_URL=http://agent_api:11436/v1  # RAG Agent
    - OLLAMA_BASE_URL=http://ollama:11434            # Direkte Ollama Modelle
    - ENABLE_OLLAMA_API=true                         # Ollama aktivieren
    - SHOW_OLLAMA_MODELS=true                        # Modelle anzeigen
    - DEFAULT_MODELS=agentic-rag                     # Standard-Modell
```

### VerfÃ¼gbare Modelle
```bash
# Ollama Modelle
docker compose exec ollama ollama list
# â†’ llama4:latest (67 GB, GPU)
# â†’ qwen2.5:14b (9.0 GB, GPU)

# RAG Agent
curl http://localhost:11436/v1/models
# â†’ agentic-rag (mit Memory + 37.624 chunks)
```

## ğŸ”§ Fehlersuche

### Falls Modelle nicht angezeigt werden:
1. **OpenWebUI neustarten:**
   ```bash
   docker compose restart openwebui
   ```

2. **Ollama Status prÃ¼fen:**
   ```bash
   docker compose exec ollama ollama ps
   ```

3. **Agent API Status prÃ¼fen:**
   ```bash
   curl http://localhost:11436/health
   ```

4. **Browser Cache leeren:**
   - Strg+F5 (Windows/Linux)
   - Cmd+Shift+R (Mac)

### Falls RAG nicht funktioniert:
1. **ChromaDB prÃ¼fen:**
   ```bash
   docker compose exec agent_api python -c "
   import chromadb
   c=chromadb.PersistentClient('/chroma')
   print('PDFs:', c.get_or_create_collection('documents').count())
   print('DOCXs:', c.get_or_create_collection('documents_docx').count())
   "
   ```

2. **Memory System prÃ¼fen:**
   ```bash
   ls -la /media/felix/RAG/AGENTIC/volumes/state/
   ```

## ğŸ“Š Performance-Vergleich

| Modell | Geschwindigkeit | QualitÃ¤t | Spezial | Anwendungsfall |
|--------|---------------|----------|----------|---------------|
| `agentic-rag` | 3-5s | â­â­â­â­â­ | ğŸ“š Dokumente | Business Fragen |
| `llama4:latest` | 2-4s | â­â­â­â­â­ | ğŸ§  Allgemein | Komplexe Aufgaben |
| `qwen2.5:14b` | 1-2s | â­â­â­â­ | âš¡ Schnell | Einfache Fragen |

## ğŸ¯ Best Practices

### 1. Richtige Modellwahl
- **Dokumenten-Fragen** â†’ Immer `agentic-rag`
- **Allgemeine Konversation** â†’ `llama4:latest`
- **Schnelle Antworten** â†’ `qwen2.5:14b`

### 2. Conversation Management
- **RAG Conversations** werden automatisch gespeichert
- **Direkte LLM Conversations** sind session-basiert
- **Wechsel zwischen Modellen** ist jederzeit mÃ¶glich

### 3. Memory Nutzung
- **RAG Agent** merkt sich frÃ¼here GesprÃ¤che
- **Conversation ID** fÃ¼r Kontext-Persistenz
- **Private Notes** fÃ¼r Agent-Working-Memory

## ğŸš€ Zukunftsoptionen

### Weitere Modelle hinzufÃ¼gen:
```bash
# Neues Modell pullen
docker compose exec ollama ollama pull model_name

# OpenWebUI neustarten
docker compose restart openwebui
```

### Custom Modelle:
- **Fine-tuned Modelle** fÃ¼r spezifische DomÃ¤nen
- **Spezialisierte Modelle** fÃ¼r bestimmte Aufgaben
- **Multi-Modal Modelle** fÃ¼r Bilder + Text

---

**ğŸ¯ Ihr E2NGIADINA RAG System unterstÃ¼tzt jetzt flexible Multi-Modell-Nutzung!**

*Web Interface: http://localhost:8086*

# üîß Agentic RAG System ‚Äì Benutzerhandbuch

> **Stand:** 2026-02-13 | **Version:** Phase 6 (ReAct Agent + Multi-Tenant)

---

## Architektur-√úberblick

```
OpenWebUI (Port 8086)
    ‚Üì OpenAI-kompatible API (SSE Streaming)
Agent API (Port 11436)  ‚Üê‚Üí  PyRunner (Port 9000)
    ‚Üì                          ‚Üë Python-Code Sandbox
    ‚îú‚Üí Elasticsearch (BM25 Keyword-Suche)
    ‚îî‚Üí ChromaDB (Vektor/Semantik-Suche)
    ‚Üì
Ollama (Port 11434) ‚Äì LLM Inference (GPU)
```

---

## 1. Modelle in OpenWebUI

In OpenWebUI gibt es zwei Gruppen von Modellen:

| Modell | Typ | Beschreibung |
|--------|-----|--------------|
| `rag-llama4:latest` | ReAct | Llama 4 mit autonomer Tool-Nutzung |
| `rag-qwen2.5:72b` | ReAct | Qwen 72B ‚Äì bestes Tool-Calling |
| `rag-llama3.3:70b` | ReAct | Llama 3.3 70B |
| `rag-gpt-oss:latest` | RAG | GPT-OSS mit klassischer Dokumentensuche |
| `llama4:latest` | Direkt | Ollama direkt, **OHNE RAG** |

### Wichtige Regel:
- **`rag-*` Modelle** ‚Üí Die Frage geht durch die RAG-Pipeline (Suche + Dokumente + LLM)
- **Modelle ohne `rag-`** ‚Üí Gehen direkt an Ollama, **keine Dokumentensuche!**

### ReAct-Modelle (empfohlen):
- `rag-llama4:latest`, `rag-qwen2.5:72b`, `rag-llama3.3:70b` nutzen den **ReAct Agent** (Pfad F)
- Der Agent entscheidet **autonom** welche Tools er braucht: Suchen, Lesen, Code ausf√ºhren, etc.
- Mehrstufige Recherche: Suchen ‚Üí Dokument lesen ‚Üí Vertiefen ‚Üí Antworten

### Thinking Mode (optional):
- `-think` Suffix (z.B. `rag-gpt-oss:latest-think`) aktiviert einen **Zwei-Schritt-Analysemodus**
- Nur n√ºtzlich f√ºr komplexe Analysefragen, wo der Denkprozess sichtbar sein soll

---

## 2. Die 6 Verarbeitungspfade

Wenn eine Frage gestellt wird, pr√ºft das System der Reihe nach:

### Pfad A: Multi-Dokument-Analyse
**Trigger:** Frage referenziert "diese Dokumente", "den Quellen", "diesen Unterlagen" etc.

Beispiele:
```
"Liste mir jeden Fehler in diesen Dokumenten auf"
"Vergleiche die Quellen miteinander"
"Was steht in allen Dokumenten zum Thema X?"
```

**Was passiert:**
1. System l√§dt die Quellen der **letzten Suche** (max. 5 Dokumente, Volltext aus ES)
2. Schickt alle Dokumente + Frage ans LLM
3. LLM analysiert exhaustiv alle Dokumente

**Voraussetzung:** Vorher muss eine Suche stattgefunden haben, deren Quellen referenziert werden.

---

### Pfad B: Einzel-Dokument-Analyse
**Trigger:** Spezifische Quelle wird mit `[N]` referenziert.

Beispiele:
```
"Analysiere Quelle [1]"
"Was steht in Dokument [3]?"
"Fasse [2] zusammen"
```

**Was passiert:**
1. System l√§dt Quelle N aus der letzten Suche (Volltext aus ES)
2. Schickt das Dokument + Frage ans LLM
3. LLM analysiert das Einzeldokument im Detail

---

### Pfad E: Transkript ‚Üí Protokoll (NEU)
**Trigger:** Keywords wie "Protokoll", "Transkript", "Pendenzen", "Whisper", "Sitzung" etc.

Beispiele:
```
"Erstelle ein Protokoll aus diesem Transkript: [Text]"
"Erstelle ein Sitzungsprotokoll aus der Datei /AuditVorbereitung.txt"
"Verarbeite dieses Meeting-Transkript zu einem Protokoll mit Pendenzenliste"
```

**Was passiert:**
1. RAG-Suche wird **komplett √ºbersprungen** ‚Äì Volltext geht direkt ans LLM
2. **Vorverarbeitung:**
   - Header-Mappings werden angewendet (z.B. `SPEAKER_00: Felix`)
   - Auto-Korrekturen f√ºr bekannte Whisper-Fehler (Adnova‚ÜíAtnova, etc.)
3. Spezialisierter **Protokoll-Prompt** generiert:
   - Sitzungskopf (Datum, Teilnehmende, Thema)
   - Traktanden mit Diskussion, Aussagen, Entscheidungen
   - Beschl√ºsse und Entscheidungen (nummeriert)
   - **Pendenzenliste** als Tabelle (Nr, Pendenz, Verantwortlich, Termin, Status)
   - N√§chste Schritte
4. **Dynamisches Context Window** (num_ctx) ‚Äì passt sich automatisch an die Textl√§nge an
5. **Dynamischer Timeout** ‚Äì bis 10 Minuten f√ºr lange Transkripte

**3 Eingabemethoden:**

| Methode | Anleitung | Qualit√§t |
|---------|-----------|----------|
| **üìé Datei-Upload** (empfohlen) | Datei in OpenWebUI hochladen + "Erstelle Protokoll" | ‚úÖ Bis ~200K Zeichen |
| **Text einf√ºgen** | Text direkt in den Chat einf√ºgen | ‚úÖ Voll |
| **Dateipfad** | `Erstelle Protokoll aus der Datei /mein_transkript.txt` | ‚úÖ Voll |

**Header-Format f√ºr Speaker-Ersetzung:**

Am Anfang der Transkript-Datei manuell hinzuf√ºgen:
```
SPEAKER_00: Felix
SPEAKER_01: Stefano

SPEAKER_00 [0.00-5.02]:
Entschuldigung, so...
```
‚Üí Alle `SPEAKER_00` werden automatisch durch `Felix` ersetzt.

---

### Pfad F: ReAct Agent ‚Äì Autonome Recherche (NEU Phase 6)
**Trigger:** Modelle `llama4:latest`, `qwen2.5:72b`, `llama3.3:70b` (automatisch)

Beispiele:
```
"Was steht im Werkvertrag √ºber Gew√§hrleistung?"
"Welche Back-to-Back Regelungen gibt es in unseren Vertr√§gen?"
"Wie viele PDF-Dateien haben wir pro Ordner?"
"Welche Ordnerstruktur haben wir?"
```

**Was passiert:**
1. Der Agent analysiert die Frage und entscheidet **autonom** welche Tools er braucht
2. **Mehrstufig:** Suchen ‚Üí Dokument lesen ‚Üí ggf. Code ausf√ºhren ‚Üí Antworten
3. Max. 6 Schritte, dann wird mit dem was vorhanden ist geantwortet
4. Quellen-Links werden automatisch am Ende angeh√§ngt

**7 verf√ºgbare Tools:**

| Tool | Beschreibung |
|------|-------------|
| `search_documents` | Hybrid-Suche (ES + Chroma) im Projektarchiv |
| `read_document` | Dokument vollst√§ndig aus ES laden |
| `execute_python` | Python-Code im Sandbox-Runner ausf√ºhren |
| `create_protocol` | Transkript ‚Üí strukturiertes Protokoll |
| `list_files` | Verzeichnisinhalt auflisten |
| `read_file` | Datei direkt lesen (CSV, TXT, Log) |
| `web_search` | Internet-Suche (braucht API-Key) |

**Forced-Step-Mechanismus:**
- Dateisystem-Fragen ("wie viele Dateien", "Ordnerstruktur") ‚Üí automatisch Python-Code
- Dokument-Fragen ‚Üí erzwungene Suche falls LLM kein Tool aufruft
- Gr√ºsse/Chat ‚Üí direkte Antwort ohne Tool

---

### Pfad C: Normaler RAG-Flow (Fallback)
**Trigger:** Jede Frage mit einem Modell das NICHT in den ReAct-Modellen ist.

Beispiele:
```
"Suche alle Manteldokumente f√ºr den GBT Z5O"
"Welche .eml Dateien gibt es?"
"Was sind die Eignungskriterien?"
```

**Ablauf Schritt f√ºr Schritt:**

1. **Glossar-Rewrite** ‚Äì Fachbegriffe werden expandiert (z.B. "GBT" ‚Üí "Gotthard Basistunnel")
2. **Query-Expansion** (nur bei Follow-ups) ‚Äì Keywords aus vorherigen Fragen werden automatisch hinzugef√ºgt
3. **Hybrid-Suche** ‚Äì Parallel in ES (Keyword/BM25) + ChromaDB (Vektor/Semantik)
4. **Ranking** ‚Äì Treffer werden nach Relevanz sortiert (Keyword-Boosting)
5. **Kontext-Aufbau** ‚Äì Top-Dokument-Snippets werden als Kontext zusammengefasst
6. **Follow-up-Kontext** (bei Nachfragen) ‚Äì Vorherige Quellen (Volltext, max. 3) werden dem Kontext vorangestellt
7. **LLM-Antwort** ‚Äì Streamt die Antwort basierend auf den Dokumenten
8. **Code-Ausf√ºhrung** (automatisch) ‚Äì Falls das LLM einen ```python Block generiert, wird dieser im Sandbox-Runner ausgef√ºhrt und das Ergebnis angeh√§ngt
9. **Quellen-Links** ‚Äì Klickbare Links zu den Quelldokumenten

---

### Pfad D: Python-Code-Ausf√ºhrung (in Pfad C integriert)
**Trigger:** Fragen die Berechnung/Z√§hlung/Dateioperationen erfordern.

Beispiele:
```
"Z√§hle alle .eml Dateien im Archiv"
"Erstelle eine Tabelle der PDF-Dateien pro Ordner"
"Berechne die Gesamtgr√∂sse aller Dokumente"
"Schreibe einen Python-Code der alle Vertr√§ge auflistet"
```

**Was passiert:**
1. Normaler RAG-Flow (Pfad C) l√§uft
2. Das LLM weiss, dass es Python schreiben kann (steht im System-Prompt)
3. Wenn es einen ```python Block generiert ‚Üí wird automatisch im **PyRunner** ausgef√ºhrt
4. PyRunner hat:
   - **Zugriff** auf das gesamte Projektarchiv (read-only) unter `/data`
   - **Bibliotheken:** `pandas`, `tabulate`, `csv`, `os`, `json`
   - **Timeout:** 25 Sekunden
   - **Kein Internet** ‚Äì nur lokale Dateien
5. Ergebnis erscheint als `üìä Ergebnis:` Block in der Antwort

**Tipp:** Wenn das LLM von sich aus keinen Code schreibt, explizit sagen: *"Schreibe Python-Code daf√ºr"*

---

## 3. Follow-up-Kontext (Gespr√§chsverlauf)

Das System merkt sich den Konversationsverlauf **innerhalb eines Chats**:

| Feature | Was passiert |
|---------|-------------|
| **Chat-History** | Die letzten 3 Frage-Antwort-Paare werden dem LLM als Kontext mitgegeben |
| **Query-Expansion** | Keywords aus vorherigen Fragen werden automatisch zur Suche hinzugef√ºgt |
| **Prev-Doc-Context** | Die Top 3 Quellen der letzten Suche werden als Volltext dem Kontext beigef√ºgt |
| **Quellen-Speicher** | Die Quellen jeder Suche werden gespeichert f√ºr "Analysiere Quelle [N]" |

### Typischer 3-Schritt-Workflow:
```
1. "Suche mir alle Manteldokumente f√ºr den GBT Z5O"
   ‚Üí Normale Suche, findet Dokumente, zeigt Quellen [1]-[5]

2. "Sind Quellen 1 bis 3 identisch?"
   ‚Üí Multi-Dokument-Analyse (Pfad A), l√§dt [1]-[3] als Volltext

3. "Liste mir jeden einzelnen Fehler in diesen Dokumenten auf"
   ‚Üí Multi-Dokument-Analyse (Pfad A), exhaustive Fehleranalyse
```

---

## 4. Dateitypen die durchsucht werden

```
md, txt, rst, log, json, yaml, yml,
pdf, docx, doc, msg, eml, .eml,
xlsx, xls, pptx, ppt
```

---

## 5. Tipps f√ºr optimale Ergebnisse

| Situation | Empfehlung |
|-----------|------------|
| **Erste Suche** | Spezifische Begriffe verwenden: *"Manteldokumente GBT Z5O"* statt *"Dokumente suchen"* |
| **Nachfragen** | Im **gleichen Chat** bleiben ‚Äì Kontext wird automatisch √ºbernommen |
| **Detailanalyse** | *"Analysiere Quelle [2] im Detail"* ‚Üí l√§dt das ganze Dokument |
| **Vergleich** | *"Vergleiche diese Dokumente"* ‚Üí Pfad A, alle vorherigen Quellen |
| **Dateien z√§hlen/listen** | Explizit *"Schreibe Python-Code"* oder *"Z√§hle alle..."* |
| **Tabellen** | *"Erstelle eine Tabelle mit..."* ‚Üí LLM kann Markdown-Tabellen oder Python/pandas nutzen |
| **Komplexe Analyse** | `-think` Modell w√§hlen ‚Üí sichtbarer Analyseschritt vor der Antwort |
| **Neues Thema** | **Neuen Chat** starten ‚Äì sonst wird alter Kontext mitgeschleppt |

---

## 6. Grenzen des Systems

- **Kein Schreiben/√Ñndern** von Dateien (nur read-only)
- **Web-Suche** nur mit API-Key (BRAVE_API_KEY oder SERPER_API_KEY in docker-compose.yml)
- **Keine Bild/Scan-Analyse** (nur Text-Extrakt aus PDFs)
- **Kein Chat-√ºbergreifendes Ged√§chtnis** ‚Äì jeder Chat ist eine eigene Session
- **Max ~12'000 Zeichen** pro Dokument im Kontext (wird gek√ºrzt bei RAG-Pfad)
- **Max 5 Dokumente** bei Multi-Dokument-Analyse
- **Max 3 vorherige Quellen** als Follow-up-Kontext
- **Datei-Upload via OpenWebUI** funktioniert bis ~200K Zeichen (RAG_TOP_K=50, CHUNK_SIZE=4000)
- **Context Window:** Dynamisch bis 128K Tokens (modellabh√§ngig)
- **ReAct Agent:** Max 6 Schritte pro Anfrage
